{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbcUSken16L"
      },
      "source": [
        "# Separate from command line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueff1zHsAwnC",
        "outputId": "8cd77fff-6013-4756-ea2f-9391b84b9b47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <generator object Estimator.predict at 0x7867d7f534c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 618, in predict\n",
            "    with tf.Graph().as_default() as g:\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5821, in get_controller\n",
            "    with super(_DefaultGraphStack,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5633, in get_controller\n",
            "    raise AssertionError(\n",
            "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:spleeter:File ./io/output/audio_example/vocals.wav written succesfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:spleeter:File ./io/output/audio_example/vocals.wav written succesfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:spleeter:File ./io/output/audio_example/accompaniment.wav written succesfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:spleeter:File ./io/output/audio_example/accompaniment.wav written succesfully\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Audio\n",
        "import os\n",
        "\n",
        "from spleeter.separator import Separator\n",
        "\n",
        "path = \"singer_voices\\singer_music\"\n",
        "for f in os.listdir(path):\n",
        "  if os.path.isfile(os.path.join(path,f)):\n",
        "    separator = Separator(\"spleeter:2stems\")\n",
        "    prediction = separator.separate_to_file(os.path.join(path,f), \"singer_voices\\singer\")\n",
        "\n",
        "# 'output' will contain the separated audio tracks, including vocals and accompaniment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## stick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pydub\n",
        "import os\n",
        "\n",
        "\n",
        "def merge_voices():\n",
        "    path = \"singer_voices/singer\"\n",
        "    two_files = []\n",
        "    for file in os.listdir(path):\n",
        "        name = file.split(\".\")[0]\n",
        "        if len(name.split(\"_\")) == 2:\n",
        "            if name.split(\"_\")[0] not in two_files:\n",
        "                two_files.append(name.split(\"_\")[0])\n",
        "                result= 0\n",
        "                for f in os.listdir(path):\n",
        "                    if f.split(\".\")[0].split(\"_\")[0] == name.split(\"_\")[0] :\n",
        "                        result += pydub.AudioSegment.from_file(f'singer_voices/singer/{f}')\n",
        "                result.export(f\"singer_voices/stick_data/{name.split('_')[0]}.wav\")\n",
        "        else:\n",
        "            result = pydub.AudioSegment.from_file(f'singer_voices/singer/{file}')\n",
        "            result.export(f\"singer_voices/stick_data/{name}.wav\", format=\"wav\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_voices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pydub\n",
        "import os\n",
        "\n",
        "def remove_silence_and_to_wav():\n",
        "    files = os.listdir('singer_voices/stick_data')\n",
        "    for file in files:\n",
        "        audio = pydub.AudioSegment.from_file(os.path.join('singer_voices/stick_data', file))\n",
        "\n",
        "        chunks = pydub.silence.split_on_silence(audio, min_silence_len=500, silence_thresh=-25)\n",
        "        result = sum(chunks)\n",
        "        file_name = file.split('.')[0]\n",
        "        result.export(f'singer_voices/rm_silence/{file_name}_rm.wav', format=\"wav\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    remove_silence_and_to_wav()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split to 1sec data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pydub\n",
        "import os\n",
        "\n",
        "\n",
        "def split_voices():\n",
        "    for file in os.listdir('singer_voices/rm_silence'):\n",
        "        audio = pydub.AudioSegment.from_file(os.path.join('singer_voices/rm_silence', file))\n",
        "        chunks = pydub.utils.make_chunks(audio, 1000)\n",
        "        person_name = file.split('.')[0]\n",
        "        os.makedirs(os.path.join('singer_voices/dataset',person_name), exist_ok=True)\n",
        "\n",
        "        for i,chunk in enumerate(chunks):\n",
        "            if len(chunk) >= 1000:\n",
        "                chunk.export(os.path.join('singer_voices/dataset',person_name, f'voice_{i}.wav'), format=\"wav\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    split_voices()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spleeter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
