{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Al52_tlNL4Ei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQE1IBpDMAk1",
        "outputId": "2f04bf04-37a2-4b05-9c0c-5d368d811f28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
              "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
              " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
              " 'frame': None,\n",
              " 'feature_names': ['pixel_0_0',\n",
              "  'pixel_0_1',\n",
              "  'pixel_0_2',\n",
              "  'pixel_0_3',\n",
              "  'pixel_0_4',\n",
              "  'pixel_0_5',\n",
              "  'pixel_0_6',\n",
              "  'pixel_0_7',\n",
              "  'pixel_1_0',\n",
              "  'pixel_1_1',\n",
              "  'pixel_1_2',\n",
              "  'pixel_1_3',\n",
              "  'pixel_1_4',\n",
              "  'pixel_1_5',\n",
              "  'pixel_1_6',\n",
              "  'pixel_1_7',\n",
              "  'pixel_2_0',\n",
              "  'pixel_2_1',\n",
              "  'pixel_2_2',\n",
              "  'pixel_2_3',\n",
              "  'pixel_2_4',\n",
              "  'pixel_2_5',\n",
              "  'pixel_2_6',\n",
              "  'pixel_2_7',\n",
              "  'pixel_3_0',\n",
              "  'pixel_3_1',\n",
              "  'pixel_3_2',\n",
              "  'pixel_3_3',\n",
              "  'pixel_3_4',\n",
              "  'pixel_3_5',\n",
              "  'pixel_3_6',\n",
              "  'pixel_3_7',\n",
              "  'pixel_4_0',\n",
              "  'pixel_4_1',\n",
              "  'pixel_4_2',\n",
              "  'pixel_4_3',\n",
              "  'pixel_4_4',\n",
              "  'pixel_4_5',\n",
              "  'pixel_4_6',\n",
              "  'pixel_4_7',\n",
              "  'pixel_5_0',\n",
              "  'pixel_5_1',\n",
              "  'pixel_5_2',\n",
              "  'pixel_5_3',\n",
              "  'pixel_5_4',\n",
              "  'pixel_5_5',\n",
              "  'pixel_5_6',\n",
              "  'pixel_5_7',\n",
              "  'pixel_6_0',\n",
              "  'pixel_6_1',\n",
              "  'pixel_6_2',\n",
              "  'pixel_6_3',\n",
              "  'pixel_6_4',\n",
              "  'pixel_6_5',\n",
              "  'pixel_6_6',\n",
              "  'pixel_6_7',\n",
              "  'pixel_7_0',\n",
              "  'pixel_7_1',\n",
              "  'pixel_7_2',\n",
              "  'pixel_7_3',\n",
              "  'pixel_7_4',\n",
              "  'pixel_7_5',\n",
              "  'pixel_7_6',\n",
              "  'pixel_7_7'],\n",
              " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
              "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
              "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
              "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
              "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
              "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
              "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
              "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
              "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
              "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
              "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
              "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
              "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
              "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
              "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
              "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
              " \n",
              "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
              "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
              "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
              "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
              " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_digits()\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvRIHJ_7MRUs",
        "outputId": "9da22a37-2c89-4335-8b1f-d69e6956f7a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gcO6zEbZDsn",
        "outputId": "a25160aa-f73e-4547-beaa-091b691e187b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "MBZqkpH0Mf89",
        "outputId": "5ae7ad4a-17fd-403c-9906-e7843759a91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1797, 8, 8)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(dataset.images.shape)\n",
        "plt.imshow(dataset.images[2],cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOpFC9f5Mzls",
        "outputId": "79689e6a-4041-490c-d7e8-a17bf9f5e8ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DHTuhZ7Q-TN",
        "outputId": "9a6e9dbd-8174-44b6-cae1-6f61ae8b927f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1797, 10)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((1437, 64), (1437, 10), (360, 64), (360, 10))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X= dataset.data\n",
        "Y=dataset.target\n",
        "Y= np.eye(10)[Y]   # one hot kardan\n",
        "print(Y.shape)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "X_train.shape , Y_train.shape, X_test.shape, Y_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8_Q33qz3RWL1"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "def root_mean_square_error(Y_gt , Y_pred):\n",
        "  return np.sqrt(np.mean((Y_gt- Y_pred)**2))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RP8tZjSSTfWu"
      },
      "outputs": [],
      "source": [
        "epochs =80\n",
        "η = 0.001   #learning rate\n",
        "\n",
        "D_in=X_train.shape[1]   # D_in = 64\n",
        "H1 = 128\n",
        "H2 = 32\n",
        "D_out = len(dataset.target_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FJLzYKIWZaC2"
      },
      "outputs": [],
      "source": [
        "W1=np.random.randn(D_in,H1)\n",
        "W2=np.random.randn(H1,H2)\n",
        "W3=np.random.randn(H2,D_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MbmEpZm4fpWI"
      },
      "outputs": [],
      "source": [
        "B1=np.random.randn(1,H1)\n",
        "B2=np.random.randn(1,H2)\n",
        "B3=np.random.randn(1,D_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So18GX6sgV0O",
        "outputId": "a77b459c-d7ff-4cfb-fe6b-f8028becb064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:  0.18232428670842032\n",
            "test:  0.25555555555555554\n",
            "train:  0.3312456506610995\n",
            "test:  0.35833333333333334\n",
            "train:  0.45789839944328464\n",
            "test:  0.4527777777777778\n",
            "train:  0.5539318023660403\n",
            "test:  0.5166666666666667\n",
            "train:  0.6193458594293667\n",
            "test:  0.5722222222222222\n",
            "train:  0.6652748782185108\n",
            "test:  0.6388888888888888\n",
            "train:  0.7174669450243563\n",
            "test:  0.6527777777777778\n",
            "train:  0.7522616562282533\n",
            "test:  0.6861111111111111\n",
            "train:  0.7780097425191371\n",
            "test:  0.7027777777777777\n",
            "train:  0.8009742519137091\n",
            "test:  0.7222222222222222\n",
            "train:  0.8128044537230341\n",
            "test:  0.7361111111111112\n",
            "train:  0.825330549756437\n",
            "test:  0.7444444444444445\n",
            "train:  0.8406402226861517\n",
            "test:  0.75\n",
            "train:  0.848990953375087\n",
            "test:  0.7583333333333333\n",
            "train:  0.8580375782881002\n",
            "test:  0.7611111111111111\n",
            "train:  0.8663883089770354\n",
            "test:  0.7694444444444445\n",
            "train:  0.8768267223382046\n",
            "test:  0.7805555555555556\n",
            "train:  0.8844815588030619\n",
            "test:  0.7861111111111111\n",
            "train:  0.8942240779401531\n",
            "test:  0.7916666666666666\n",
            "train:  0.8970076548364648\n",
            "test:  0.7972222222222223\n",
            "train:  0.9004871259568545\n",
            "test:  0.8\n",
            "train:  0.9046624913013221\n",
            "test:  0.7972222222222223\n",
            "train:  0.9130132219902575\n",
            "test:  0.8027777777777778\n",
            "train:  0.9199721642310369\n",
            "test:  0.8055555555555556\n",
            "train:  0.9227557411273486\n",
            "test:  0.8138888888888889\n",
            "train:  0.9269311064718163\n",
            "test:  0.825\n",
            "train:  0.929714683368128\n",
            "test:  0.8277777777777777\n",
            "train:  0.9311064718162839\n",
            "test:  0.825\n",
            "train:  0.9324982602644398\n",
            "test:  0.8305555555555556\n",
            "train:  0.9345859429366736\n",
            "test:  0.8416666666666667\n",
            "train:  0.9380654140570633\n",
            "test:  0.8444444444444444\n",
            "train:  0.9380654140570633\n",
            "test:  0.8472222222222222\n",
            "train:  0.9401530967292971\n",
            "test:  0.8472222222222222\n",
            "train:  0.942240779401531\n",
            "test:  0.8472222222222222\n",
            "train:  0.9450243562978428\n",
            "test:  0.8527777777777777\n",
            "train:  0.9471120389700766\n",
            "test:  0.8527777777777777\n",
            "train:  0.9478079331941545\n",
            "test:  0.8555555555555555\n",
            "train:  0.9498956158663883\n",
            "test:  0.85\n",
            "train:  0.954070981210856\n",
            "test:  0.85\n",
            "train:  0.9547668754349339\n",
            "test:  0.8527777777777777\n",
            "train:  0.9561586638830898\n",
            "test:  0.8527777777777777\n",
            "train:  0.9582463465553236\n",
            "test:  0.8527777777777777\n",
            "train:  0.9582463465553236\n",
            "test:  0.8527777777777777\n",
            "train:  0.9610299234516354\n",
            "test:  0.8527777777777777\n",
            "train:  0.9617258176757133\n",
            "test:  0.8527777777777777\n",
            "train:  0.9617258176757133\n",
            "test:  0.8527777777777777\n",
            "train:  0.9617258176757133\n",
            "test:  0.8527777777777777\n",
            "train:  0.9631176061238692\n",
            "test:  0.8527777777777777\n",
            "train:  0.965205288796103\n",
            "test:  0.8527777777777777\n",
            "train:  0.9665970772442589\n",
            "test:  0.8527777777777777\n",
            "train:  0.9665970772442589\n",
            "test:  0.8555555555555555\n",
            "train:  0.9665970772442589\n",
            "test:  0.8527777777777777\n",
            "train:  0.9665970772442589\n",
            "test:  0.8527777777777777\n",
            "train:  0.9679888656924147\n",
            "test:  0.8527777777777777\n",
            "train:  0.9679888656924147\n",
            "test:  0.8583333333333333\n",
            "train:  0.9679888656924147\n",
            "test:  0.8583333333333333\n",
            "train:  0.9693806541405706\n",
            "test:  0.8583333333333333\n",
            "train:  0.9693806541405706\n",
            "test:  0.8555555555555555\n",
            "train:  0.9700765483646486\n",
            "test:  0.8555555555555555\n",
            "train:  0.9700765483646486\n",
            "test:  0.8583333333333333\n",
            "train:  0.9714683368128044\n",
            "test:  0.8583333333333333\n",
            "train:  0.9728601252609603\n",
            "test:  0.8583333333333333\n",
            "train:  0.9728601252609603\n",
            "test:  0.8611111111111112\n",
            "train:  0.9735560194850382\n",
            "test:  0.8611111111111112\n",
            "train:  0.9742519137091162\n",
            "test:  0.8611111111111112\n",
            "train:  0.9742519137091162\n",
            "test:  0.8638888888888889\n",
            "train:  0.9749478079331941\n",
            "test:  0.8694444444444445\n",
            "train:  0.9749478079331941\n",
            "test:  0.8722222222222222\n",
            "train:  0.9756437021572721\n",
            "test:  0.875\n",
            "train:  0.97633959638135\n",
            "test:  0.875\n",
            "train:  0.9770354906054279\n",
            "test:  0.8722222222222222\n",
            "train:  0.977731384829506\n",
            "test:  0.875\n",
            "train:  0.977731384829506\n",
            "test:  0.8777777777777778\n",
            "train:  0.9784272790535838\n",
            "test:  0.8777777777777778\n",
            "train:  0.9784272790535838\n",
            "test:  0.8805555555555555\n",
            "train:  0.9784272790535838\n",
            "test:  0.8805555555555555\n",
            "train:  0.9791231732776617\n",
            "test:  0.8833333333333333\n",
            "train:  0.9791231732776617\n",
            "test:  0.8833333333333333\n",
            "train:  0.9805149617258176\n",
            "test:  0.8805555555555555\n",
            "train:  0.9805149617258176\n",
            "test:  0.8805555555555555\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  Y_PRED_train=[]\n",
        "  Y_PRED_test=[]\n",
        "\n",
        "  # train\n",
        "\n",
        "  for x , y in zip(X_train,Y_train):\n",
        "\n",
        "    x = x.reshape(-1,1)\n",
        "    #forward\n",
        "\n",
        "    # layer1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "    # layer2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "    # layer3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    Y_pred = out3\n",
        "    Y_PRED_train.append(Y_pred)\n",
        "\n",
        "    loss = root_mean_square_error(y , Y_pred)\n",
        "    # backward - we should calculate moshtaq\n",
        "\n",
        "    # layer3\n",
        "    error = -2 * (y - Y_pred)\n",
        "    grad_B3 = error\n",
        "    grad_W3 = out2.T @ error\n",
        "\n",
        "    # layer2\n",
        "    error = error @ W3.T * out2 * (1 - out2)\n",
        "    grad_B2 = error\n",
        "    grad_W2 = out1.T @ error\n",
        "\n",
        "    # layer1\n",
        "    error = error @W2.T * out1 * (1 - out1)\n",
        "    grad_B1 = error\n",
        "    grad_W1 =  x @error\n",
        "\n",
        "\n",
        "    # update\n",
        "\n",
        "    #layer1\n",
        "    W1 -= η * grad_W1\n",
        "    B1 -= η * grad_B1\n",
        "\n",
        "    #layer2\n",
        "    W2 -= η * grad_W2\n",
        "    B2 -= η * grad_B2\n",
        "\n",
        "\n",
        "    #layer3\n",
        "    W3 -= η * grad_W3\n",
        "    B3 -= η * grad_B3\n",
        "\n",
        "    # acc= ...\n",
        "    # loss= ...\n",
        "\n",
        "\n",
        "  for x , y in zip(X_test,Y_test):\n",
        "\n",
        "\n",
        "    x = x.reshape(-1,1)\n",
        "    #forward\n",
        "\n",
        "    # layer1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "    # layer2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "    # layer3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    Y_pred = out3\n",
        "    Y_PRED_test.append(Y_pred)\n",
        "\n",
        "  Y_PRED_train = np.array(Y_PRED_train).reshape(-1,10)\n",
        "  loss_train = root_mean_square_error(Y_train, Y_PRED_train)\n",
        "  accuracy_train =np.sum(np.argmax(Y_train,axis=1)==np.argmax(Y_PRED_train,axis=1))/len(Y_PRED_train)\n",
        "  print(\"train: \", accuracy_train)\n",
        "\n",
        "\n",
        "  Y_PRED_test = np.array(Y_PRED_test).reshape(-1,10)\n",
        "  loss_test = root_mean_square_error(Y_test, Y_PRED_test)\n",
        "  accuracy_test =np.sum(np.argmax(Y_test,axis=1)==np.argmax(Y_PRED_test,axis=1))/len(Y_PRED_test)\n",
        "  print(\"test: \",accuracy_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TPcbxNVn44DP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "image = cv2.imread(\"input/2.jpg\")\n",
        "image = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
        "image = image.reshape(64,1)\n",
        "\n",
        "x= image\n",
        "\n",
        "#forward\n",
        "\n",
        "# layer1\n",
        "out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "# layer2\n",
        "out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "# layer3\n",
        "out3 = softmax(out2 @ W3 + B3)\n",
        "Y_pred = out3\n",
        "print(np.argmax(Y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
